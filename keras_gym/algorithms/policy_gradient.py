import numpy as np

from ..losses import BasePolicyLoss
from ..utils import ExperienceCache
from ..policies.base import BaseActorCritic

from .base import BasePolicyAlgorithm


class Reinforce(BasePolicyAlgorithm):
    """
    Update the policy according to the REINFORCE algorithm, cf. Section 13.3 of
    `Sutton & Barto <http://incompleteideas.net/book/the-book-2nd.html>`_.

    Parameters
    ----------
    policy : updateable policy

        An updateable policy object, see :mod:`keras_gym.policies`.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, policy, gamma=0.9, experience_cache_size=0):
        self._episode_cache = ExperienceCache(overflow='grow')
        super().__init__(policy, gamma, experience_cache_size)

    def update(self, s, a, r, s_next, done):
        """
        Update the policy.

        Parameters
        ----------
        s : int or array

            A single observation (state).

        a : int or array

            A single action.

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        done : bool

            Whether the episode is done. If ``done`` is ``False``, the input
            transition is cached and no actual update will take place. Once
            ``done`` is ``True``, however, the collected cache from the episode
            is unrolled, replaying the epsiode in reverse chronological order.
            This is when the actual updates are made.

        """
        X, A, R, X_next = self.preprocess_transition(s, a, r, s_next)
        X_next = np.zeros_like(X_next)
        I_next = np.zeros(1)
        self._episode_cache.append(X, A, R, X_next)

        # break out of function if episode hasn't yet finished
        if not done:
            return

        # initialize return
        G = 0

        # replay episode in reverse order
        while self._episode_cache:
            X, A, R, X_next = self._episode_cache.pop()

            # use (non-centered) return G as recorded advantages
            G = R + self.gamma * G

            # keep experience
            if self.experience_cache is not None:
                self.experience_cache.append(X, A, G, X_next, I_next)

            self.policy.update(X, A, G)


class AdvantageActorCritic(BaseActorCritic):
    """
    Modular implementation of Advantage Actor-Critic (A2C).

    This isn't an algorithm on its own. Instead, it's a wrapper that combines a
    policy (actor) with a value function (critic) into a single object. This
    actor-critic object can be passed to any V-type algorithm to update the
    critic. The actor is then automatically trained alongside the critic.

    Parameters
    ----------

    policy : updateable policy

        An updateable policy object, see :mod:`keras_gym.policies`.

    value_function : state value function

        State-value function :math:`V(s)`.

    train_model : keras.Model, optional

        A single model to update both the actor and the critic in one call to
        ``train_on_batch``. See :class:`LinearSoftmaxActorCritic
        <keras_gym.predefined.linear.LinearSoftmaxActorCritic>` for an example
        implementation that uses a ``train_model``.

    Examples
    --------

    The way to train using A2C is to contruct an actor-critic and to pass it to
    a V-type algorithm. Consider the following simple example:

    .. code:: python

        import gym
        from keras_gym.predefined.linear import LinearV, LinearSoftmaxPolicy
        from keras_gym.algorithms import AdvantageActorCritic, NStepBootstrap

        env = gym.make(...)

        actor_critic = AdvantageActorCritic(
            policy=LinearSoftmaxPolicy(env, lr=0.01),
            value_function=LinearV(env, lr=0.1))

        a2c_algo = NStepBootstrap(actor_critic, n=10)

        # usual boilerplate to run env episodes
        ...

        # within an episode, update the actor-critic by feeding a transition
        a = ac.policy.thompson(s)
        s_next, r, done, info = env.step(a)
        a2c_algo.update(s, a, r, s_next, done)


    """
    def set_policy_loss(self, loss_function_class, **kwargs):
        """
        Change the policy loss function.

        Parameters
        ----------

        loss_function_class : keras-compatible loss function

            The provided policy loss must have a subclass of
            :class:`BasePolicyLoss <keras_gym.losses.BasePolicyLoss>`.

        **kwargs : keyword arguments

            Any additional arguments to be passed to the loss-function class.

        """
        if self.train_model is not None:
            raise NotImplementedError(
                "Cannot change the policy loss function when a custom "
                "`train_model` is present.")
        if not issubclass(loss_function_class, BasePolicyLoss):
            raise TypeError(
                "`loss_function_class` must be a subclass of BasePolicyLoss")

        advantages = self.policy.model.loss.advantages  # assume attr exists
        self.policy.model.compile(
            loss=loss_function_class(advantages, **kwargs),  # replace loss
            optimizer=self.policy.model.optimizer,
            metrics=self.policy.model.metrics,
            loss_weights=self.policy.model.loss_weights,
            sample_weight_mode=self.policy.model.sample_weight_mode,
            weighted_metrics=self.policy.model.weighted_metrics,
            target_tensors=self.policy.model.target_tensors,
        )
