import numpy as np

from ..utils import ExperienceCache
from ..policies import GenericActorCritic

from .base import BasePolicyAlgorithm, BaseAlgorithm


class Reinforce(BasePolicyAlgorithm):
    """
    Update the policy according to the REINFORCE algorithm, cf. Section 13.3 of
    `Sutton & Barto <http://incompleteideas.net/book/the-book-2nd.html>`_.

    Parameters
    ----------
    policy : updateable policy

        An updateable policy object, see :mod:`keras_gym.policies`.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, policy, gamma=0.9, experience_cache_size=0):
        self._episode_cache = ExperienceCache(overflow='grow')
        super().__init__(policy, gamma, experience_cache_size)

    def update(self, s, a, r, s_next, done):
        """
        Update the policy.

        Parameters
        ----------
        s : int or array

            A single observation (state).

        a : int or array

            A single action.

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        done : bool

            Whether the episode is done. If ``done`` is ``False``, the input
            transition is cached and no actual update will take place. Once
            ``done`` is ``True``, however, the collected cache from the episode
            is unrolled, replaying the epsiode in reverse chronological order.
            This is when the actual updates are made.

        """
        X, A, R, X_next = self.preprocess_transition(s, a, r, s_next)
        X_next = np.zeros_like(X_next)
        I_next = np.zeros(1)
        self._episode_cache.append(X, A, R, X_next)

        # break out of function if episode hasn't yet finished
        if not done:
            return

        # initialize return
        G = 0

        # replay episode in reverse order
        while self._episode_cache:
            X, A, R, X_next = self._episode_cache.pop()

            # use (non-centered) return G as recorded advantages
            G = R + self.gamma * G

            # keep experience
            if self.experience_cache is not None:
                self.experience_cache.append(X, A, G, X_next, I_next)

            self.policy.update(X, A, G)


class AdvantageActorCritic(BaseAlgorithm):
    """
    Implementation of the advantage actor-critic (A2C) algorithm.

    In A2C, we learn both a policy :math:`\\hat{\\pi}(a|s)` (actor) as
    well as a state value function :math:`\\hat{v}(s)` (critic).

    This algorithm either takes an actor-critic object, see e.g.
    :class:`GenericActorCritic <keras_gym.policies.GenericActorCritic>` or it
    takes both the policy and value function as separate arguments.

    Parameters
    ----------
    actor_critic : actor-critic object

        This is usually just a wrapper that bundles the policy and value
        function into one object.

    value_function_algo_class : value function algorithm

        The algorithm to use for updating the value function :math:`V(s)`.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    gamma : float

        Future discount factor, value between 0 and 1.

    **algo_kwargs : keyword arguments

        Any additional keyword arguments to pass to the
        ``value_function_algo_class`` when instantiating the value-function
        algorithm.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    value_function_algo : value-function algorithm instance

        This is instantiated as follows:

            .. code:: python

                value_function_algo = value_function_algo_class(
                    actor_critic.value_function,
                    gamma=gamma, experience_cache_size=experience_cache_size,
                    **algo_kwargs)

    """
    def __init__(self, actor_critic, value_function_algo_class, gamma=0.9,
                 experience_cache_size=0, **algo_kwargs):

        self.actor_critic = actor_critic
        self.value_function_algo = value_function_algo_class(
            actor_critic.value_function,
            gamma=gamma, experience_cache_size=experience_cache_size,
            **algo_kwargs)

    @classmethod
    def from_policy_and_value_function(cls, policy, value_function,
                                       value_function_algo_class, gamma=0.9,
                                       experience_cache_size=0,
                                       **algo_kwargs):
        """
        Create an instance from separate policy and value-function objects
        instead of a combined actor-critic object.

        Parameters
        ----------
        policy : policy object

            An updateable policy object.

        value_function : value-function object

            An value function representing :math:`V(s)`.

        value_function_algo_class : value function algorithm

            The algorithm to use for updating the value function :math:`V(s)`.

        experience_cache_size : positive int, optional

            If provided, we populate a presisted experience cache that can be
            used for (asynchronous) experience replay. If left unspecified, no
            experience_cache is created. The specific value depends on your
            application. If you pick a value that's too big you might have
            issues coming from the fact early samples are less representative
            of the data generated by the current policy. Of course, there are
            physical limitations too. If you pick a value that's too small you
            might also end up with a sample that's insufficiently
            representative. So, the right value balances negative effects from
            remembering too much and forgetting too quickly.

        gamma : float

            Future discount factor, value between 0 and 1.

        **algo_kwargs : keyword arguments

            Any additional keyword arguments to pass to the
            ``value_function_algo_class`` when instantiating the value-function
            algorithm.

        """
        actor_critic = GenericActorCritic(policy, value_function)
        self = cls(actor_critic, value_function_algo_class, gamma,
                   experience_cache_size, **algo_kwargs)
        return self

    def update(self, s, a, r, s_next, done):
        """
        Update the underlying actor-critic object.

        Parameters
        ----------
        s : state observation

            A single state observation.

        a : action

            A single action a

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        s_next : state observation

            A single state observation. This is the state for which we will
            compute the estimated future return, i.e. bootstrapping.

        done : bool

            Whether the episode is done. If ``done`` is ``False``, the input
            transition is cached and no actual update will take place. Once
            ``done`` is ``True``, however, the collected cache from the episode
            is unrolled, replaying the epsiode in reverse chronological order.
            This is when the actual updates are made.

        """
        #TODO: implement
