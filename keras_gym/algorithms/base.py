from abc import ABC, abstractmethod
from copy import deepcopy

import numpy as np
from tensorflow import keras

from ..utils import ExperienceCache
from ..value_functions import GenericV, GenericQ, GenericQTypeII
from ..policies.base import BaseUpdateablePolicy, BaseActorCritic


class BaseAlgorithm(ABC):
    """
    Abstract base class for all algorithm objects.

    Parameters
    ----------
    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    target_func_update_delay : non-negative int, optional

        If a non-zero value is provided, the function approximator
        (:class:`keras.Model`) is copied. The copy of the model is often called
        *target* function approximator. The specific value provided for
        ``target_func_update_delay`` specifies the number of observations
        (i.e. *not* the number of batches) to wait before updating
        the target function approximator.

    target_func_update_tau : float, optional

        If there is a target function approximator present, this parameter
        specifies how "hard" the update must be. The update rule is:

        .. math::

            w_\\text{target}\\ \\leftarrow\\ (1-\\tau)\\,w_\\target + \\tau\\,w

        where :math:`w` (without subscript) are the weights of the model that
        is continually updated. A hard update is accomplished by to the default
        value :math:`tau=1`.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    target_func : value function or policy

        A copy of the underlying value function or policy. This is used to
        compute bootstrap targets. This model is typically only updated
        periodically; the period being set by the ``target_func_update_delay``
        parameter.

    """
    def __init__(self, gamma=0.9, experience_cache_size=0,
                 experience_replay_batch_size=0, target_func_update_delay=0,
                 target_func_update_tau=1.0):

        self.gamma = gamma

        # experience replay for stabilizing updates
        self.experience_cache = None
        self.experience_replay_batch_size = int(experience_replay_batch_size)

        if experience_cache_size:
            self.experience_cache = ExperienceCache(
                maxlen=int(experience_cache_size), overflow='cycle')
        if int(experience_cache_size) < self.experience_replay_batch_size:
            raise ValueError(
                "`experience_cache_size` must be at least at large as "
                "`experience_replay_batch_size`")

        # target model for stabilizing bootstrap updates
        self.target_func = None
        self.target_func_update_delay = int(target_func_update_delay)
        self.target_func_update_tau = float(target_func_update_tau)

        if not (0.0 <= self.target_func_update_tau <= 1.0):
            raise ValueError(
                "target_func_update_tau must lie on the unit interval")
        if self.target_func_update_delay:
            self._func.bootstrap_model = None
            self.target_func = deepcopy(self._func)
            self.target_func.model = keras.models.clone_model(self._func.model)
            self.target_func.model.set_weights(self._func.model.get_weights())

    @property
    def _func(self):
        if hasattr(self, 'value_function'):
            return self.value_function
        if hasattr(self, 'policy'):
            return self.policy
        raise AttributeError(
            "algorithm has neither a value_function nor a policy")

    def _check_update_target_model(self, num_observations):
        if not hasattr(self, '_update_counter'):
            self._update_counter = 0
        self._update_counter += int(num_observations)
        n = self.target_func_update_delay
        if self.target_func is not None and self._update_counter % n == 0:
            tau = self.target_func_update_tau
            fresh_weights = self._func.model.get_weights()
            old_target_weights = self.target_func.get_weights()
            new_target_weights = [
                (1 - tau) * w_old + tau * w
                for w, w_old in zip(fresh_weights, old_target_weights)]
            self.target_func.set_weights(new_target_weights)

    def preprocess_transition(self, s, a, r, s_next):
        """
        Prepare a single transition to be used for policy updates or experience
        caching.

        Parameters
        ----------
        s : state observation

            A single state observation.

        a : action

            A single action a

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        s_next : state observation

            A single state observation. This is the state for which we will
            compute the estimated future return, i.e. bootstrapping.

        Returns
        -------
        X, A, R, X_next : arrays

            Preprocessed versions of the inputs (s, a, r, s_next).

        """
        if hasattr(self, 'value_function'):
            is_policy = False
            fn = self.value_function
        elif hasattr(self, 'policy'):
            is_policy = True
            fn = self.policy
        else:
            raise AttributeError(
                "cannot find either `value_fntion` or `policy` attribute")

        X = self._preprocess_X(fn, s, a)
        A = np.array([a])
        R = np.array([r])
        X_next = fn.X(s_next) if is_policy else fn.X_next(s_next)

        # check shapes
        assert X.shape == (1, fn.input_dim), "bad shape"
        assert A.shape == (1,), "bad shape"
        assert R.shape == (1,), "bad shape"
        if isinstance(fn, GenericQ):
            shape = (1, fn.num_actions, fn.input_dim)
            assert X_next.shape == shape, "bad shape"
        elif isinstance(fn, (GenericV, GenericQTypeII, BaseUpdateablePolicy)):
            assert X_next.shape == (1, fn.input_dim), "bad shape"
        else:
            raise ValueError("unexpected value-function/policy type")

        return X, A, R, X_next

    @staticmethod
    def _preprocess_X(fn, s, a):
        """ This is a little helper method to avoid duplication of code. """
        if isinstance(fn, GenericQ):
            return fn.X(s, a)
        elif isinstance(fn, (GenericV, GenericQTypeII, BaseUpdateablePolicy)):
            return fn.X(s)
        else:
            raise ValueError("unexpected value-function type")

    @abstractmethod
    def update(self, s, a, r, s_next, done):
        """
        Update the given value function.

        Parameters
        ----------
        s : state observation

            A single state observation.

        a : action

            A single action a

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        s_next : state observation

            A single state observation. This is the state for which we will
            compute the estimated future return, i.e. bootstrapping.

        done : bool

            Whether the episode has finished.

        """
        pass


class BaseVAlgorithm(BaseAlgorithm):
    """
    Abstract base class for algorithms that update a state value function
    :math:`V(s)`.

    Parameters
    ----------
    value_function_or_actor_critic : value function or actor-critic object

        Either a state value function :math:`V(s)` or an actor-critic object.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : non-negative int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    experience_replay_batch_size : non-negative int, optional

        If ``experience_cache_size > 0``, this setting sets the batch size of
        the experience-replay updates. If this is left unspecified, basic
        per-observation updates are done instead.

    target_func_update_delay : non-negative int, optional

        If a non-zero value is provided, the function approximator
        (:class:`keras.Model`) is copied. The copy of the model is often called
        *target* function approximator. The specific value provided for
        ``target_func_update_delay`` specifies the number of observations
        (i.e. *not* the number of batches) to wait before updating
        the target function approximator.

    target_func_update_tau : float, optional

        If there is a target function approximator present, this parameter
        specifies how "hard" the update must be. The update rule is:

        .. math::

            w_\\text{target}\\ \\leftarrow\\ (1-\\tau)\\,w_\\target + \\tau\\,w

        where :math:`w` (without subscript) are the weights of the model that
        is continually updated. A hard update is accomplished by to the default
        value :math:`tau=1`.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    target_func : value function or policy

        A copy of the underlying value function or policy. This is used to
        compute bootstrap targets. This model is typically only updated
        periodically; the period being set by the ``target_func_update_delay``
        parameter.

    """
    def __init__(self, value_function_or_actor_critic, gamma=0.9,
                 experience_cache_size=0, experience_replay_batch_size=0,
                 target_func_update_delay=0, target_func_update_tau=1.0):

        type_V = (
            isinstance(value_function_or_actor_critic, GenericV)
            and not isinstance(value_function_or_actor_critic, GenericQTypeII))  # noqa: W503, E501
        type_AC = (
            isinstance(value_function_or_actor_critic, BaseActorCritic))

        if type_V:
            self.actor_critic = None
            self.value_function = value_function_or_actor_critic
        elif type_AC:
            self.actor_critic = value_function_or_actor_critic
            self.value_function = self.actor_critic.value_function
        else:
            raise ValueError(
                "expected a state value function or an actor-critic object")

        super().__init__(
            gamma, experience_cache_size, experience_replay_batch_size)

    def _update_value_function_or_actor_critic(self, X, A, Gn, X_next, I_next):
        """ This is a little helper method to avoid duplication of code. """

        if self.actor_critic is not None:
            self.actor_critic.update(X, A, Gn, X_next, I_next)
        else:
            if self.target_func is not None:
                V_next = self.target_func.batch_eval_next(X_next)
            else:
                V_next = self.value_function.batch_eval_next(X_next)
            G = Gn + I_next * V_next
            self.value_function.update(X, G)


class BaseQAlgorithm(BaseAlgorithm):
    """
    Abstract base class for algorithms that update a state-action value
    function :math:`Q(s, a)`.

    Parameters
    ----------
    value_function : state-action value function

        A state-action value function :math:`Q(s, a)`.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    experience_replay_batch_size : non-negative int, optional

        If ``experience_cache_size > 0``, this setting sets the batch size of
        the experience-replay updates. If this is left unspecified, basic
        per-observation updates are done instead.

    target_func_update_delay : non-negative int, optional

        If a non-zero value is provided, the function approximator
        (:class:`keras.Model`) is copied. The copy of the model is often called
        *target* function approximator. The specific value provided for
        ``target_func_update_delay`` specifies the number of observations
        (i.e. *not* the number of batches) to wait before updating
        the target function approximator.

    target_func_update_tau : float, optional

        If there is a target function approximator present, this parameter
        specifies how "hard" the update must be. The update rule is:

        .. math::

            w_\\text{target}\\ \\leftarrow\\ (1-\\tau)\\,w_\\target + \\tau\\,w

        where :math:`w` (without subscript) are the weights of the model that
        is continually updated. A hard update is accomplished by to the default
        value :math:`tau=1`.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    target_func : value function or policy

        A copy of the underlying value function or policy. This is used to
        compute bootstrap targets. This model is typically only updated
        periodically; the period being set by the ``target_func_update_delay``
        parameter.

    """
    def __init__(self, value_function, gamma=0.9, experience_cache_size=0,
                 experience_replay_batch_size=0, target_func_update_delay=0,
                 target_func_update_tau=1.0):

        if not isinstance(value_function, (GenericQ, GenericQTypeII)):
            raise ValueError("expected a Q-type value function")
        self.value_function = value_function
        super().__init__(
            gamma, experience_cache_size, experience_replay_batch_size)

    def _update_value_function(self, X, A, G):
        """ This is a little helper method to avoid duplication of code. """
        if isinstance(self.value_function, GenericQ):
            self.value_function.update(X, G)
        elif isinstance(self.value_function, GenericQTypeII):
            self.value_function.update(X, A, G)  # project onto actions
        else:
            raise ValueError("unexpected value-function type")


class BasePolicyAlgorithm(BaseAlgorithm):
    """
    Abstract base class for algorithms that update a value function
    :math:`V(s)` or :math:`Q(s,a)`.

    Parameters
    ----------
    policy : policy object
        A policy object. Can be either a value-based policy model or a direct
        policy-gradient model.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    experience_replay_batch_size : non-negative int, optional

        If ``experience_cache_size > 0``, this setting sets the batch size of
        the experience-replay updates. If this is left unspecified, basic
        per-observation updates are done instead.

    target_func_update_delay : non-negative int, optional

        If a non-zero value is provided, the function approximator
        (:class:`keras.Model`) is copied. The copy of the model is often called
        *target* function approximator. The specific value provided for
        ``target_func_update_delay`` specifies the number of observations
        (i.e. *not* the number of batches) to wait before updating
        the target function approximator.

    target_func_update_tau : float, optional

        If there is a target function approximator present, this parameter
        specifies how "hard" the update must be. The update rule is:

        .. math::

            w_\\text{target}\\ \\leftarrow\\ (1-\\tau)\\,w_\\target + \\tau\\,w

        where :math:`w` (without subscript) are the weights of the model that
        is continually updated. A hard update is accomplished by to the default
        value :math:`tau=1`.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    target_func : value function or policy

        A copy of the underlying value function or policy. This is used to
        compute bootstrap targets. This model is typically only updated
        periodically; the period being set by the ``target_func_update_delay``
        parameter.

    """
    def __init__(self, policy, gamma=0.9, experience_cache_size=0,
                 experience_replay_batch_size=0, target_func_update_delay=0,
                 target_func_update_tau=1.0):

        if not isinstance(policy, BaseUpdateablePolicy):
            raise TypeError("expected a policy object")
        self.policy = policy
        super().__init__(
            gamma, experience_cache_size, experience_replay_batch_size)
