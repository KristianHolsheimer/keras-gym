from abc import ABC, abstractmethod
import numpy as np

from ..utils import ExperienceCacheV2
from ..value_functions import GenericV, GenericQ, GenericQTypeII
from ..policies.base import BaseUpdateablePolicy, BaseActorCritic


class BaseAlgorithm(ABC):
    """
    Abstract base class for all algorithm objects.

    Parameters
    ----------
    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, gamma=0.9, experience_cache_size=0,
                 experience_replay_batch_size=0):

        self.gamma = gamma
        self.experience_cache = None
        self.experience_replay_batch_size = int(experience_replay_batch_size)
        if experience_cache_size:
            self.experience_cache = ExperienceCacheV2(
                maxlen=int(experience_cache_size))
        if int(experience_cache_size) < self.experience_replay_batch_size:
            raise ValueError(
                "`experience_cache_size` must be at least at large as "
                "`experience_replay_batch_size`")

    def preprocess_transition(self, s, a, r, s_next):
        """
        Prepare a single transition to be used for policy updates or experience
        caching.

        Parameters
        ----------
        s : state observation

            A single state observation.

        a : action

            A single action a

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        s_next : state observation

            A single state observation. This is the state for which we will
            compute the estimated future return, i.e. bootstrapping.

        Returns
        -------
        X, A, R, X_next : arrays

            Preprocessed versions of the inputs (s, a, r, s_next).

        """
        if hasattr(self, 'value_function'):
            is_policy = False
            fn = self.value_function
        elif hasattr(self, 'policy'):
            is_policy = True
            fn = self.policy
        else:
            raise AttributeError(
                "cannot find either `value_fntion` or `policy` attribute")

        X = self._preprocess_X(fn, s, a)
        A = np.array([a])
        R = np.array([r])
        X_next = fn.X(s_next) if is_policy else fn.X_next(s_next)

        # check shapes
        assert X.shape == (1, fn.input_dim), "bad shape"
        assert A.shape == (1,), "bad shape"
        assert R.shape == (1,), "bad shape"
        if isinstance(fn, GenericQ):
            shape = (1, fn.num_actions, fn.input_dim)
            assert X_next.shape == shape, "bad shape"
        elif isinstance(fn, (GenericV, GenericQTypeII, BaseUpdateablePolicy)):
            assert X_next.shape == (1, fn.input_dim), "bad shape"
        else:
            raise ValueError("unexpected value-function/policy type")

        return X, A, R, X_next

    @staticmethod
    def _preprocess_X(fn, s, a):
        """ This is a little helper method to avoid duplication of code. """
        if isinstance(fn, GenericQ):
            return fn.X(s, a)
        elif isinstance(fn, (GenericV, GenericQTypeII, BaseUpdateablePolicy)):
            return fn.X(s)
        else:
            raise ValueError("unexpected value-function type")

    @abstractmethod
    def update(self, s, a, r, s_next, done):
        """
        Update the given value function.

        Parameters
        ----------
        s : state observation

            A single state observation.

        a : action

            A single action a

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        s_next : state observation

            A single state observation. This is the state for which we will
            compute the estimated future return, i.e. bootstrapping.

        done : bool

            Whether the episode has finished.

        """
        pass


class BaseVAlgorithm(BaseAlgorithm):
    """
    Abstract base class for algorithms that update a state value function
    :math:`V(s)`.

    Parameters
    ----------
    value_function_or_actor_critic : value function or actor-critic object

        Either a state value function :math:`V(s)` or an actor-critic object.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    experience_replay_batch_size : positive int, optional

        If provided, we do experience-replay updates instead of regular, single
        instance updates.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, value_function_or_actor_critic, gamma=0.9,
                 experience_cache_size=0, experience_replay_batch_size=0):

        type_V = (
            isinstance(value_function_or_actor_critic, GenericV)
            and not isinstance(value_function_or_actor_critic, GenericQTypeII))  # noqa: W503, E501
        type_AC = (
            isinstance(value_function_or_actor_critic, BaseActorCritic))

        if type_V:
            self.actor_critic = None
            self.value_function = value_function_or_actor_critic
        elif type_AC:
            self.actor_critic = value_function_or_actor_critic
            self.value_function = self.actor_critic.value_function
        else:
            raise ValueError(
                "expected a state value function or an actor-critic object")

        super().__init__(
            gamma, experience_cache_size, experience_replay_batch_size)

    def _update_value_function_or_actor_critic(self, X, A, Gn, X_next, I_next):
        """ This is a little helper method to avoid duplication of code. """

        if self.actor_critic is not None:
            self.actor_critic.update(X, A, Gn, X_next, I_next)
        # elif self.value_function.bootstrap_model is not None:
        #     self.value_function.update_bootstrapped(
        #         X, Gn, X_next, I_next)
        else:
            V_next = self.value_function.batch_eval_next(X_next)
            G = Gn + I_next * V_next
            self.value_function.update(X, G)


class BaseQAlgorithm(BaseAlgorithm):
    """
    Abstract base class for algorithms that update a state-action value
    function :math:`Q(s, a)`.

    Parameters
    ----------
    value_function : state-action value function

        A state-action value function :math:`Q(s, a)`.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    experience_replay_batch_size : positive int, optional

        If provided, we do experience-replay updates instead of regular, single
        instance updates.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, value_function, gamma=0.9, experience_cache_size=0,
                 experience_replay_batch_size=0):

        if not isinstance(value_function, (GenericQ, GenericQTypeII)):
            raise ValueError("expected a Q-type value function")
        self.value_function = value_function
        super().__init__(
            gamma, experience_cache_size, experience_replay_batch_size)

    def _update_value_function(self, X, A, G):
        """ This is a little helper method to avoid duplication of code. """
        if isinstance(self.value_function, GenericQ):
            self.value_function.update(X, G)
        elif isinstance(self.value_function, GenericQTypeII):
            self.value_function.update(X, A, G)  # project onto actions
        else:
            raise ValueError("unexpected value-function type")


class BasePolicyAlgorithm(BaseAlgorithm):
    """
    Abstract base class for algorithms that update a value function
    :math:`V(s)` or :math:`Q(s,a)`.

    Parameters
    ----------
    policy : policy object
        A policy object. Can be either a value-based policy model or a direct
        policy-gradient model.

    gamma : float

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    experience_replay_batch_size : positive int, optional

        If provided, we do experience-replay updates instead of regular, single
        instance updates.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, policy, gamma=0.9, experience_cache_size=0,
                 experience_replay_batch_size=0):

        if not isinstance(policy, BaseUpdateablePolicy):
            raise TypeError("expected a policy object")
        self.policy = policy
        super().__init__(
            gamma, experience_cache_size, experience_replay_batch_size)
