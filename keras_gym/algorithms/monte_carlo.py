import numpy as np

from ..utils import ExperienceCache

from .base import BaseVAlgorithm, BaseQAlgorithm


class MonteCarloV(BaseVAlgorithm):
    """
    Update the Q-function according to the plain vanilla Monte Carlo algorithm,
    cf. Section 5.3 of `Sutton & Barto
    <http://incompleteideas.net/book/the-book-2nd.html>`_.

    Parameters
    ----------
    value_function_or_actor_critic : value function or actor-critic object

        Either a state value function :math:`V(s)` or an actor-critic object.

    gamma : float, optional

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, value_function_or_actor_critic, gamma=0.9,
                 experience_cache_size=0):

        self._episode_cache = ExperienceCache(overflow='grow')
        super().__init__(
            value_function_or_actor_critic, gamma, experience_cache_size)

    def update(self, s, a, r, s_next, done):
        X, A, R, X_next = self.preprocess_transition(s, a, r, s_next)
        X_next = np.zeros_like(X_next)
        I_next = np.zeros(1)

        self._episode_cache.append(X, A, R, X_next, I_next)

        # break out of function if episode hasn't yet finished
        if not done:
            return

        # initialize return
        G = 0

        # replay episode in reverse order
        while self._episode_cache:
            X, A, R, X_next, I_next = self._episode_cache.pop()
            G = R + self.gamma * G

            # keep experience
            if self.experience_cache is not None:
                self.experience_cache.append(X, A, R, X_next, I_next)

            # update
            if self.actor_critic is not None:
                self.actor_critic.update(X, A, G, X_next, I_next)
            else:
                self.value_function.update(X, G)


class MonteCarloQ(BaseQAlgorithm):
    """
    Update the Q-function according to the plain vanilla Monte Carlo algorithm,
    cf. Section 5.3 of `Sutton & Barto
    <http://incompleteideas.net/book/the-book-2nd.html>`_.

    Parameters
    ----------
    value_function : state-action value function

        A state value function :math:`Q(s, a)`.

    gamma : float

        Future discount factor, value between 0 and 1.

    """
    def __init__(self, value_function, gamma=0.9):
        self._episode_cache = ExperienceCache(overflow='grow')
        super(MonteCarloQ, self).__init__(value_function, gamma=gamma)

    def update(self, s, a, r, s_next, done):
        """
        Update the value function.

        Parameters
        ----------
        s : int or array

            A single observation (state).

        a : int or array

            A single action.

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        done : bool

            Whether the episode is done. If ``done`` is ``False``, the input
            transition is cached and no actual update will take place. Once
            ``done`` is ``True``, however, the collected cache from the episode
            is unrolled, replaying the epsiode in reverse chronological order.
            This is when the actual updates are made.

        """
        X, A, R, X_next = self.preprocess_transition(s, a, r, s_next)
        self._episode_cache.append(X, A, R)

        # break out of function if episode hasn't yet finished
        if not done:
            return

        # initialize return
        G = 0

        # replay episode in reverse order
        while self._episode_cache:
            X, A, R, X_next, I_next = self._episode_cache.pop()
            G = R + self.gamma * G

            # keep experience
            if self.experience_cache is not None:
                self.experience_cache.append(X, A, R, X_next, I_next)

            # update
            self._update_value_function(X, A, G)
