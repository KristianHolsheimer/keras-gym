import numpy as np

from ..utils import ExperienceCache, accumulate_rewards

from .base import BaseVAlgorithm, BaseQAlgorithm


class MonteCarloV(BaseVAlgorithm):
    """
    Update the Q-function according to the plain vanilla Monte Carlo algorithm,
    cf. Section 5.3 of `Sutton & Barto
    <http://incompleteideas.net/book/the-book-2nd.html>`_.

    Parameters
    ----------
    value_function_or_actor_critic : value function or actor-critic object

        Either a state value function :math:`V(s)` or an actor-critic object.

    batch_update : bool, optional

        Whether to perform the updates in batch (entire episode). If not, the
        updates are processed one timestep at a time.

    gamma : float, optional

        Future discount factor, value between 0 and 1.

    experience_cache_size : positive int, optional

        If provided, we populate a presisted experience cache that can be used
        for (asynchronous) experience replay. If left unspecified, no
        experience_cache is created. The specific value depends on your
        application. If you pick a value that's too big you might have issues
        coming from the fact early samples are less representative of the data
        generated by the current policy. Of course, there are physical
        limitations too. If you pick a value that's too small you might also
        end up with a sample that's insufficiently representative. So, the
        right value balances negative effects from remembering too much and
        forgetting too quickly.

    Attributes
    ----------
    experience_cache : ExperienceCache or None

        The persisted experience cache, which could be used for (asynchronous)
        experience-replay type updates.

    """
    def __init__(self, value_function_or_actor_critic, batch_update=False,
                 gamma=0.9, experience_cache_size=0):
        self.batch_update = batch_update
        self._episode_cache = ExperienceCache(overflow='grow')
        super().__init__(
            value_function_or_actor_critic, gamma, experience_cache_size)

    def update(self, s, a, r, s_next, done):
        X, A, R, X_next = self.preprocess_transition(s, a, r, s_next)
        self._episode_cache.append(X, A, R)

        # break out of function if episode hasn't yet finished
        if not done:
            return

        if self.batch_update:

            # get data from cache
            X = self._episode_cache[0].array
            R = self._episode_cache[1].array

            # create target
            G = accumulate_rewards(R, self.gamma)

            # batch update (play batch in reverse)
            self.value_function.update(X, G)

            # clear cache for next episode
            self._episode_cache.clear()

        else:

            # initialize return
            G = 0

            # replay episode in reverse order
            while self._episode_cache:
                X, R = self._episode_cache.pop()

                # gamma-discounted return
                G = R + self.gamma * G

                self.value_function.update(X, G)


class MonteCarloQ(BaseQAlgorithm):
    """
    Update the Q-function according to the plain vanilla Monte Carlo algorithm,
    cf. Section 5.3 of `Sutton & Barto
    <http://incompleteideas.net/book/the-book-2nd.html>`_.

    Parameters
    ----------
    value_function : state-action value function

        A state value function :math:`Q(s, a)`.

    batch_update : bool, optional

        Whether to perform the updates in batch (entire episode). If not, the
        updates are processed one timestep at a time.

    gamma : float

        Future discount factor, value between 0 and 1.

    """
    def __init__(self, value_function, batch_update=False, gamma=0.9):
        self.batch_update = batch_update
        self._episode_cache = ExperienceCache(overflow='grow')
        super(MonteCarloQ, self).__init__(value_function, gamma=gamma)

    def update(self, s, a, r, s_next, done):
        """
        Update the value function.

        Parameters
        ----------
        s : int or array

            A single observation (state).

        a : int or array

            A single action.

        r : float

            Reward associated with the transition
            :math:`(s, a)\\to s_\\text{next}`.

        done : bool

            Whether the episode is done. If ``done`` is ``False``, the input
            transition is cached and no actual update will take place. Once
            ``done`` is ``True``, however, the collected cache from the episode
            is unrolled, replaying the epsiode in reverse chronological order.
            This is when the actual updates are made.

        """
        X, A, R, X_next = self.preprocess_transition(s, a, r, s_next)
        self._episode_cache.append(X, A, R)

        # break out of function if episode hasn't yet finished
        if not done:
            return

        if self.batch_update:

            # get data from cache
            X = self._episode_cache[0].array
            A = self._episode_cache[1].array
            R = self._episode_cache[2].array

            # create target
            G = accumulate_rewards(R, self.gamma)

            # batch update
            self._update_value_function(X, A, G)

            # clear cache for next episode
            self._episode_cache.clear()

        else:

            # initialize return
            G = 0

            # replay episode in reverse order
            while self._episode_cache:
                X, A, R = self._episode_cache.pop()

                # gamma-discounted return
                G = R + self.gamma * G

                # update
                self._update_value_function(X, A, G)
