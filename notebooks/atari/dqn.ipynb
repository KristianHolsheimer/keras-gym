{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-gym -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../scripts/atari/dqn.py\n",
    "import gym\n",
    "import keras_gym as km\n",
    "\n",
    "\n",
    "# env with preprocessing\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = km.wrappers.ImagePreprocessor(env, height=105, width=80, grayscale=True)\n",
    "env = km.wrappers.FrameStacker(env, num_frames=3)\n",
    "env = km.wrappers.TrainMonitor(env)\n",
    "\n",
    "# show logs from TrainMonitor\n",
    "km.enable_logging()\n",
    "\n",
    "\n",
    "# value function\n",
    "func = km.predefined.AtariFunctionApproximator(env, lr=0.00025)\n",
    "q = km.QTypeII(\n",
    "    func, gamma=0.99, bootstrap_n=1, bootstrap_with_target_model=True)\n",
    "buffer = km.caching.ExperienceReplayBuffer.from_value_function(\n",
    "    q, capacity=1000000, batch_size=32)\n",
    "policy = km.EpsilonGreedy(q)\n",
    "\n",
    "\n",
    "# exploration schedule\n",
    "def epsilon(T):\n",
    "    \"\"\" stepwise linear annealing \"\"\"\n",
    "    M = 1000000\n",
    "    if T < M:\n",
    "        return 1 - 0.9 * T / M\n",
    "    if T < 2 * M:\n",
    "        return 0.1 - 0.09 * (T - M) / M\n",
    "    return 0.01\n",
    "\n",
    "\n",
    "# static parameters\n",
    "buffer_warmup_period = 50000\n",
    "target_model_sync_period = 10000\n",
    "\n",
    "\n",
    "while env.T < 3000000:\n",
    "    if env.ep % 10 == 0 and env.T > buffer_warmup_period:\n",
    "        km.utils.generate_gif(\n",
    "            env=env,\n",
    "            policy=policy.set_epsilon(0.01),\n",
    "            filepath='./data/dqn/gifs/ep{:06d}.gif'.format(env.ep),\n",
    "            resize_to=(320, 420))\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        policy.epsilon = epsilon(env.T)\n",
    "        a = policy(s)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "\n",
    "        buffer.add(s, a, r, done, env.ep)\n",
    "\n",
    "        if env.T > buffer_warmup_period:\n",
    "            q.batch_update(*buffer.sample())\n",
    "\n",
    "        if env.T % target_model_sync_period == 0:\n",
    "            q.sync_target_model()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
