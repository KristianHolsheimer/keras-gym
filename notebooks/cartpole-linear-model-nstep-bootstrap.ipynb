{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Contents\n",
    "\n",
    "- [N-Step Q-learning](#N-Step-Q-learning)\n",
    "- [N-Step SARSA](#N-Step-SARSA)\n",
    "- [N-Step Expected-SARSA](#N-Step-Expected-SARSA)\n",
    "\n",
    "\n",
    "# N-Step Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed after 16 steps\n",
      "failed after 82 steps\n",
      "failed after 30 steps\n",
      "failed after 145 steps\n",
      "num_consecutive_successes = 1\n",
      "failed after 111 steps\n",
      "failed after 11 steps\n",
      "failed after 77 steps\n",
      "failed after 37 steps\n",
      "num_consecutive_successes = 1\n",
      "failed after 159 steps\n",
      "num_consecutive_successes = 1\n",
      "num_consecutive_successes = 2\n",
      "num_consecutive_successes = 3\n",
      "num_consecutive_successes = 4\n",
      "num_consecutive_successes = 5\n",
      "num_consecutive_successes = 6\n",
      "num_consecutive_successes = 7\n",
      "failed after 180 steps\n",
      "num_consecutive_successes = 1\n",
      "num_consecutive_successes = 2\n",
      "num_consecutive_successes = 3\n",
      "num_consecutive_successes = 4\n",
      "num_consecutive_successes = 5\n",
      "num_consecutive_successes = 6\n",
      "num_consecutive_successes = 7\n",
      "num_consecutive_successes = 8\n",
      "num_consecutive_successes = 9\n",
      "num_consecutive_successes = 10\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from keras_gym.value_functions import LinearQ\n",
    "from keras_gym.policies import ValueBasedPolicy\n",
    "from keras_gym.algorithms import NStepQLearning\n",
    "\n",
    "\n",
    "# the Gym environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "# define Q, its induced policy and update algorithm\n",
    "Q = LinearQ(env, interaction='elementwise_quadratic', lr=0.01, momentum=0.2)\n",
    "policy = ValueBasedPolicy(Q)\n",
    "algo = NStepQLearning(Q, n=20, gamma=0.8)\n",
    "\n",
    "\n",
    "# number of iterations\n",
    "num_episodes = 200\n",
    "max_episode_steps = env._max_episode_steps\n",
    "\n",
    "\n",
    "# used for early stopping\n",
    "num_consecutive_successes = 0\n",
    "\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    last_episode = episode == num_episodes or num_consecutive_successes == 9\n",
    "    \n",
    "    # init\n",
    "    s = env.reset()\n",
    "    \n",
    "    # amount of random exploration\n",
    "    if last_episode:\n",
    "        epsilon = 0\n",
    "        env.render()\n",
    "    elif episode < 10:\n",
    "        epsilon = 0.5\n",
    "    else:\n",
    "        epsilon = 0.01\n",
    "    \n",
    "    for t in range(1, max_episode_steps + 1):\n",
    "        a = policy.epsilon_greedy(s, epsilon)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        \n",
    "        # update or render\n",
    "        if not last_episode:\n",
    "            algo.update(s, a, r, s_next, done)            \n",
    "        else:\n",
    "            env.render()\n",
    "        \n",
    "        # keep track of consecutive successes\n",
    "        if done:\n",
    "            if t == max_episode_steps:\n",
    "                num_consecutive_successes += 1\n",
    "                print(f\"num_consecutive_successes = {num_consecutive_successes}\")\n",
    "            else:\n",
    "                num_consecutive_successes = 0\n",
    "                print(f\"failed after {t} steps\")\n",
    "            break\n",
    "    \n",
    "        # prepare for next step\n",
    "        s = s_next\n",
    "\n",
    "        \n",
    "    if last_episode:\n",
    "        break\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed after 11 steps\n",
      "failed after 13 steps\n",
      "failed after 37 steps\n",
      "failed after 136 steps\n",
      "failed after 86 steps\n",
      "failed after 45 steps\n",
      "failed after 156 steps\n",
      "failed after 143 steps\n",
      "failed after 56 steps\n",
      "num_consecutive_successes = 1\n",
      "num_consecutive_successes = 2\n",
      "num_consecutive_successes = 3\n",
      "num_consecutive_successes = 4\n",
      "num_consecutive_successes = 5\n",
      "num_consecutive_successes = 6\n",
      "num_consecutive_successes = 7\n",
      "num_consecutive_successes = 8\n",
      "num_consecutive_successes = 9\n",
      "num_consecutive_successes = 10\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from keras_gym.value_functions import GenericQ\n",
    "from keras_gym.policies import ValueBasedPolicy\n",
    "from keras_gym.algorithms import NStepSarsa\n",
    "\n",
    "\n",
    "# the Gym environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "\n",
    "# define Q, its induced policy and update algorithm\n",
    "Q = LinearQ(env, interaction='elementwise_quadratic', lr=0.05, momentum=0.1, decay=0.05)\n",
    "policy = ValueBasedPolicy(Q)\n",
    "algo = NStepSarsa(Q, n=20, gamma=0.8)\n",
    "\n",
    "\n",
    "# number of iterations\n",
    "num_episodes = 200\n",
    "max_episode_steps = env._max_episode_steps\n",
    "\n",
    "\n",
    "# used for early stopping\n",
    "num_consecutive_successes = 0\n",
    "\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    last_episode = episode == num_episodes or num_consecutive_successes == 9\n",
    "    \n",
    "    # init\n",
    "    s = env.reset()\n",
    "    a = policy.random()\n",
    "    \n",
    "    # amount of random exploration\n",
    "    if last_episode:\n",
    "        epsilon = 0\n",
    "        env.render()\n",
    "    elif episode < 10:\n",
    "        epsilon = 0.5\n",
    "    else:\n",
    "        epsilon = 0.01\n",
    "    \n",
    "    for t in range(1, max_episode_steps + 1):\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        a_next = policy.epsilon_greedy(s_next, epsilon)\n",
    "        \n",
    "        # update or render\n",
    "        if not last_episode:\n",
    "            algo.update(s, a, r, s_next, a_next, done)            \n",
    "        else:\n",
    "            env.render()\n",
    "        \n",
    "        # keep track of consecutive successes\n",
    "        if done:\n",
    "            if t == max_episode_steps:\n",
    "                num_consecutive_successes += 1\n",
    "                print(f\"num_consecutive_successes = {num_consecutive_successes}\")\n",
    "            else:\n",
    "                num_consecutive_successes = 0\n",
    "                print(f\"failed after {t} steps\")\n",
    "            break\n",
    "    \n",
    "        # prepare for next step\n",
    "        s, a = s_next, a_next\n",
    "\n",
    "        \n",
    "    if last_episode:\n",
    "        break\n",
    "\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
